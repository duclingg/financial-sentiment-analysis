{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Sentiment Analysis (Single)\n",
    "\n",
    "In this program, I run a sentiment analysis of a single company based on financial news articles.\n",
    "\n",
    "The company that I am targeting is Nvidia [NVDA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching News Articles\n",
    "\n",
    "The first step is to fetch the news articles.  \n",
    "\n",
    "I am using `NewsAPI` to get articles quickly and easily. Then, I use `pandas` to put the articles into a dataframe, where I can collect and read the data easier.  \n",
    "\n",
    "**Filtering articles:**  \n",
    "Filter articles that only exist  \n",
    "- `NewsAPI` sometimes fetches articles that were removed  \n",
    "\n",
    "**Extracting the data:**  \n",
    "Extract only the necessary data from the articles\n",
    "- Title\n",
    "- Description\n",
    "- Content\n",
    "\n",
    "All others can be discarded.  \n",
    "\n",
    "Both of these steps are part of the cleaning data step that is next in text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get path to the environment file\n",
    "env_path = '../config/.env'\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import newsapi package\n",
    "from newsapi import NewsApiClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init newsapi\n",
    "newsapi = NewsApiClient(api_key=os.getenv('NEWS_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all articles that mention Nvidia\n",
    "all_articles = newsapi.get_everything(q='Nvidia',\n",
    "                                      language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title\n",
      "0  DOJ subpoenas NVIDIA as part of antitrust prob...\n",
      "1                                          [Removed]\n",
      "2  Nvidia might actually lose in this key part of...\n",
      "3  Nvidia CEO Jensen Huang says the payback on AI...\n",
      "4  Stock market today: Dow hits record high while...\n"
     ]
    }
   ],
   "source": [
    "# place all_articles into a dataframe\n",
    "all_articles_df = pd.DataFrame(all_articles['articles'])\n",
    "print(all_articles_df[['title']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter articles function\n",
    "# only filters valid articles\n",
    "# valid meaning: article exists and description of article exists\n",
    "def filter_removed_articles(articles):\n",
    "    return [article for article in articles if article.get('title') != '[Removed]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the all_articles\n",
    "valid_articles = filter_removed_articles(all_articles['articles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title\n",
      "0  DOJ subpoenas NVIDIA as part of antitrust prob...\n",
      "1  Nvidia might actually lose in this key part of...\n",
      "2  Nvidia CEO Jensen Huang says the payback on AI...\n",
      "3  Stock market today: Dow hits record high while...\n",
      "4  Nvidia Hit With DOJ Subpoena In Escalating Ant...\n"
     ]
    }
   ],
   "source": [
    "valid_articles_df = pd.DataFrame(valid_articles)\n",
    "print(valid_articles_df[['title']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract article essentials function\n",
    "# extract only the title, description, and content from the articles\n",
    "def extract_article_essentials(articles):\n",
    "    return [{'title': article['title'], 'descripton': article['description'], 'content': article['content']} for article in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_articles = extract_article_essentials(valid_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title\n",
      "0  DOJ subpoenas NVIDIA as part of antitrust prob...\n",
      "1  Nvidia might actually lose in this key part of...\n",
      "2  Nvidia CEO Jensen Huang says the payback on AI...\n",
      "3  Stock market today: Dow hits record high while...\n",
      "4  Nvidia Hit With DOJ Subpoena In Escalating Ant...\n"
     ]
    }
   ],
   "source": [
    "extracted_articles_df = pd.DataFrame(extracted_articles)\n",
    "print(extracted_articles_df[['title']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text\n",
    "***This is a crucial***  \n",
    "Proprocessing helps clean and normalize the text data making it more suitable for analysis.  \n",
    "\n",
    "After getting the articles, I can now preprocess the text in the articles.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning \n",
    "**Identify and remove noise:**  \n",
    "We want to first remove all noise from the data.  \n",
    "- Punction\n",
    "- Extra whitespace\n",
    "\n",
    "**Text normalization:**  \n",
    "- Stopwords\n",
    "- Capital letters\n",
    "    - All letters should be the same case so all words are treated the same in the tokenization process.  \n",
    "\n",
    "**Data masking:**  \n",
    "Data masking is not needed in this context.  \n",
    "\n",
    "Clean text should result.  \n",
    "\n",
    "**Tokenization:**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re package (regular expressions)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk packages (Natural Language Toolkit)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/justinhoang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download nltk data package 'stopwords'\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text function\n",
    "# cleans the data (text)\n",
    "def clean_text(text):\n",
    "    # identify and remove noise\n",
    "    # remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # normalize text\n",
    "    # lower case all text\n",
    "    text = text.lower()\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # tokenize words\n",
    "    \n",
    "    \n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
