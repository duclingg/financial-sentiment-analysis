{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Sentiment Analysis (Single)\n",
    "\n",
    "In this program, I run a sentiment analysis of a single company based on financial news articles.\n",
    "\n",
    "The company that I am targeting is Nvidia [NVDA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching News Articles\n",
    "\n",
    "The first step is to fetch the news articles.  \n",
    "\n",
    "I am using `NewsAPI` to get articles quickly and easily. Then, I use `pandas` to put the articles into a dataframe, where I can collect and read the data easier.  \n",
    "\n",
    "**Filtering articles:**  \n",
    "Filter articles that only exist  \n",
    "- `NewsAPI` sometimes fetches articles that were removed  \n",
    "\n",
    "**Extracting the data:**  \n",
    "Extract only the necessary data from the articles\n",
    "- Title\n",
    "- Description\n",
    "- Content\n",
    "\n",
    "All others can be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from newsapi.newsapi_client import NewsApiClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init newsapi\n",
    "newsapi = NewsApiClient(api_key=os.getenv('NEWS_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch articles\n",
    "all_articles = newsapi.get_everything(q='Nvidia',\n",
    "                                      language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.3'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title\n",
      "0  DOJ subpoenas NVIDIA as part of antitrust prob...\n",
      "1                                          [Removed]\n",
      "2  Nvidia might actually lose in this key part of...\n",
      "3  Nvidia CEO Jensen Huang says the payback on AI...\n",
      "4  Stock market today: Dow hits record high while...\n"
     ]
    }
   ],
   "source": [
    "all_articles_df = pd.DataFrame(all_articles['articles'])\n",
    "print(all_articles_df[['title']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter articles that are valid only\n",
    "# valid meaning: article exists and description of article exists\n",
    "def filter_removed_articles(articles):\n",
    "    return [article for article in articles if article.get('title') != '[Removed]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_articles = filter_removed_articles(all_articles['articles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title\n",
      "0  DOJ subpoenas NVIDIA as part of antitrust prob...\n",
      "1  Nvidia might actually lose in this key part of...\n",
      "2  Nvidia CEO Jensen Huang says the payback on AI...\n",
      "3  Stock market today: Dow hits record high while...\n",
      "4  Nvidia Hit With DOJ Subpoena In Escalating Ant...\n"
     ]
    }
   ],
   "source": [
    "valid_articles_df = pd.DataFrame(valid_articles)\n",
    "print(valid_articles_df[['title']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the title, description, and content from the articles\n",
    "def extract_article_essentials(articles):\n",
    "    return [{'title': article['title'], 'descripton': article['description'], 'content': article['content']} for article in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = extract_article_essentials(valid_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title\n",
      "0  DOJ subpoenas NVIDIA as part of antitrust prob...\n",
      "1  Nvidia might actually lose in this key part of...\n",
      "2  Nvidia CEO Jensen Huang says the payback on AI...\n",
      "3  Stock market today: Dow hits record high while...\n",
      "4  Nvidia Hit With DOJ Subpoena In Escalating Ant...\n"
     ]
    }
   ],
   "source": [
    "# create a data frame of the articles with essentials only\n",
    "articles_df = pd.DataFrame(articles)\n",
    "print(articles_df[['title']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text\n",
    "\n",
    "***This is a crucial***  \n",
    "Proprocessing helps clean and standarize the text data making it more suitable for analysis.  \n",
    "\n",
    "After getting the articles, I can now preprocess the text in the articles.  \n",
    "\n",
    "#### Remove noise\n",
    "We want to first remove all noise from the data.  \n",
    "This includes punction and capital letters\n",
    "- All letters should be the same case so all words are treated the same in the tokenization process.\n",
    "\n",
    "#### Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk package (Natural Language Toolkit)\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download nltk data downloader\n",
    "nltk.download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
